{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, append=True)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1988)\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(1988)\n",
    "torch.set_deterministic(True)\n",
    "\n",
    "import random \n",
    "random.seed(1988) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar los archivos y crear 2 listas.\n",
    "# reviews, con las lineas de texto\n",
    "# labels,  con la correspondiente clase\n",
    "\n",
    "with open('./raw/badqueries.unix', 'r', encoding='utf-8') as dataset_file:\n",
    "    bads = dataset_file.readlines()\n",
    "with open('./raw/goodqueries.unix', 'r', encoding='utf-8') as dataset_file:\n",
    "    goods = dataset_file.readlines()\n",
    "    \n",
    "# Concateno las listas    \n",
    "reviews = bads + goods\n",
    "# Creo 2 listas con [1|0] de acuerdo al tamanio de cada dataset\n",
    "labels  = ([1] * len(bads)) + ([0] * len(goods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de Registros Malos [48126]\n",
      "Cantidad de Registros Buenos [1294531]\n",
      "Cantidad de Registros Registros Totales [1342657]\n",
      "Cantidad de Registros Labels Totales [1342657]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de Registros Malos [{}]\".format(len(bads)))\n",
    "print(\"Cantidad de Registros Buenos [{}]\".format(len(goods)))\n",
    "print(\"Cantidad de Registros Registros Totales [{}]\".format(len(reviews)))\n",
    "print(\"Cantidad de Registros Labels Totales [{}]\".format(len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# Importamos los simbolos de puntuacion para poder eliminarlos de los registros\n",
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un array con todas las palabras del dataset completo\n",
    "# Se eliminan todos los posibles simbolos de puntuacion y se dejan \n",
    "# lo que son palabras / numeros\n",
    "all_reviews=list()\n",
    "for text in reviews:\n",
    "  text = text.lower()\n",
    "  for a in punctuation:\n",
    "      text = text.replace(a, ' ')\n",
    "  all_reviews.append(text)\n",
    "all_text = \" \".join(all_reviews)\n",
    "all_words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "\n",
    "# Vamos a contar todas las palabras que hay contando el metodo counter\n",
    "count_words  = Counter(all_words)      # Array con el distinct de palabras\n",
    "total_words  = len(all_words)          # Numero total de palabras en el datase\n",
    "sorted_words = count_words.most_common(total_words) # Lista ordenadas por cantidad de ocurrencias de palabras unicas \n",
    "\n",
    "# Al final obtenemos un dict en donde el key es la palabra y el valor es numerador de palabra\n",
    "# Esta lista la vamos a usar luego para reemplazar las palabras por su numero dentro de las \n",
    "# posibles palabras y usar los numeros en el tensor\n",
    "vocab_to_int={w:i+1 for i,(w,c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de Palabras Diferentes    [780630]\n",
      "Cantidad de Palabras En el dataset [3238625]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de Palabras Diferentes    [{}]\".format(len(count_words)))\n",
    "print(\"Cantidad de Palabras En el dataset [{}]\".format(total_words))\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El objetivo es pasar la lista inicial de palabras del dataset y reemplazar la palabra\n",
    "# por su valor numerico de palabra del diccionario de transformacion\n",
    "# Al final encoded_reviews va a tener una lista con las lineas del dataset encoded\n",
    "# en formato numerico\n",
    "\n",
    "encoded_reviews=list()\n",
    "for review in all_reviews:\n",
    "  encoded_review=list()\n",
    "  for word in review.split():\n",
    "    if word not in vocab_to_int.keys():\n",
    "      encoded_review.append(0) # Si la palabra no existe poner 0 para que no de error\n",
    "    else:\n",
    "      encoded_review.append(vocab_to_int[word])\n",
    "  encoded_reviews.append(encoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora vamos a crear un vector, con la cantidad constante de numeros para todas las lineas\n",
    "# Esto porque este va a ser la entrada a la red y tiene que ser un numero constante\n",
    "\n",
    "sequence_length=300\n",
    "features=np.zeros((len(encoded_reviews), sequence_length), dtype=int)\n",
    "for i, review in enumerate(encoded_reviews):\n",
    "  review_len=len(review)\n",
    "  if (review_len<=sequence_length):\n",
    "    zeros=list(np.zeros(sequence_length-review_len))\n",
    "    new=zeros+review\n",
    "  else:\n",
    "    new=review[:sequence_length]\n",
    "  features[i,:]=np.array(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de Elementos [1074125] Numero de Clase 0 [1035675] Perc [0.9642034213895031]\n",
      "Numero de Elementos [268532] Numero de Clase 0 [258856] Perc [0.9639670504818793]\n"
     ]
    }
   ],
   "source": [
    "# Split los dataset, 80% para training y 20% para test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(features, labels, test_size=0.2, random_state=666)\n",
    "\n",
    "# La distribucion de clases se tiene que mantener (96% Clase 0)\n",
    "print(\"Numero de Elementos [{}] Numero de Clase 0 [{}] Perc [{}]\".format(len(train_y), train_y.count(0), train_y.count(0)/len(train_y)))\n",
    "print(\"Numero de Elementos [{}] Numero de Clase 0 [{}] Perc [{}]\".format(len(valid_y), valid_y.count(0), valid_y.count(0)/len(valid_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#create Tensor Dataset\n",
    "train_data=TensorDataset(torch.FloatTensor(train_x), torch.FloatTensor(train_y))\n",
    "valid_data=TensorDataset(torch.FloatTensor(valid_x), torch.FloatTensor(valid_y))\n",
    "\n",
    "#dataloader\n",
    "batch_size=32\n",
    "trainloader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "validloader=DataLoader(valid_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):  \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = torch.nn.Linear(in_features, out_features)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)\n",
    "        probs = F.relu(logits)  \n",
    "        return probs, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (fc): Linear(in_features=300, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "classifier = Classifier(in_features = 300, out_features = 2)\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optim = optim.SGD(params = classifier.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33567/33567 [00:40<00:00, 830.76it/s]\n",
      "100%|██████████| 33567/33567 [00:39<00:00, 860.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "iter     = 0\n",
    "epochs   = 2\n",
    "\n",
    "acc_list  = list()\n",
    "loss_list = list()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in tqdm(trainloader):\n",
    "        iter += 1\n",
    "        \n",
    "        classifier.train()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # Predict sentiment probabilities\n",
    "        probs, logits = classifier(inputs)\n",
    "\n",
    "        target  = torch.tensor(labels, requires_grad=True)\n",
    "        loss    = (( probs.argmax(dim=1) -  target)**2).sum()\n",
    "        \n",
    "        preds   = probs.argmax(dim=1)\n",
    "        targets = target\n",
    "        \n",
    "        accuracy = (preds == targets).sum().float()\n",
    "        accuracy = 100 * (accuracy / batch_size)\n",
    "        acc_list.append(accuracy)\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        writer.add_scalar('train/loss', loss,     iter )\n",
    "        writer.add_scalar('train/acc',  accuracy, iter )     \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8392/8392 [00:07<00:00, 1105.08it/s]\n"
     ]
    }
   ],
   "source": [
    "val_iter = 0\n",
    "acc_val = list()\n",
    "loss_val = list()\n",
    "classifier.eval()\n",
    "for inputs, labels in tqdm(validloader):\n",
    "    val_iter =+ 1\n",
    "\n",
    "    probs, logits = classifier(inputs)\n",
    "    loss = ((probs.argmax(dim=1) -  labels)**2).sum().float()\n",
    "\n",
    "    preds   = probs.argmax(dim=1)\n",
    "    targets = labels\n",
    "\n",
    "    accuracy = preds.eq(targets).sum().float()\n",
    "    accuracy = 100 * (accuracy / batch_size)\n",
    "\n",
    "    # Log to tensorboard\n",
    "    writer.add_scalar('val/loss', loss,     val_iter)\n",
    "    writer.add_scalar('val/acc',  accuracy, val_iter)\n",
    "    acc_val.append(accuracy)\n",
    "    loss_val.append(loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en Validacion [94.30521392822266]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy en Validacion [{}]\".format((sum(acc_val)/len(acc_val)).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33566\n"
     ]
    }
   ],
   "source": [
    "aaa = 0\n",
    "for iter, (vectors, targets) in enumerate(trainloader):\n",
    "    aaa = iter\n",
    "print(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67133"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch * len(trainloader) + aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
